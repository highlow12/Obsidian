[웹사이트](https://swharden.com/blog/2024-02-19-local-ai-chat-csharp/)

해당 웹사이트는 Scott W Harden이 작성한 "Local AI Chat with C#"이라는 제목의 블로그 포스트입니다. 이 포스트에서는 C#을 사용하여 로컬에서 LLaMA 2 대규모 언어 모델([[LLM]])을 실행하여 AI 채팅을 구현하고, 로컬 문서에 대한 질문에 답변하는 방법에 대해 설명하고 있습니다.

이전에는 Python을 사용하여 LLaMA2를 로컬에서 실행하고 문서에 대한 질문에 답변하는 방법을 설명했지만, Python 개발 환경을 설정하고 유지하는 것이 어려울 수 있기 때문에 .NET 환경을 사용하여 비슷한 기능을 구현하는 방법을 소개하고 있습니다.

이 포스트에서는 무료 오픈소스 도구를 사용하여 로컬에서 호스팅되는 LLM을 활용하는 C# 애플리케이션을 만드는 방법을 보여줍니다. 이 애플리케이션은 대화형 채팅을 제공하며, 로컬 문서의 정보를 검색, 요약, 질문에 답변하는 기능을 포함하고 있습니다.

포스트에서는 다음과 같은 주요 리소스를 언급하고 있습니다:

- LLaMA (Large Language Model Meta AI): 메타에서 만든 LLM 가족입니다.
- HuggingFace: LLaMA 모델의 다양한 버전을 호스팅하는 웹사이트입니다. 이 프로젝트에서는 GGUF 형식의 모델을 사용합니다.
- llama.cpp: LLM과 상호 작용하는 간단한 API를 제공하는 오픈소스 C++ 프로젝트입니다.
- LLamaSharp: llama.cpp에 대한 간단한 .NET 인터페이스를 제공하는 오픈소스 프로젝트입니다. 이 프로젝트는 llama.cpp 프로젝트와 함께 발전하며 새로운 기능과 성능 향상을 포함합니다.
- KernelMemory: LLM을 사용하여 문서를 색인화하고 요청에 따라 정보를 검색하는 고수준 AI 서비스를 제공하는 오픈소스 프로젝트입니다.

이 포스트의 요약(TLDR)은 다음과 같습니다:

- HuggingFace에서 GGUF 파일을 다운로드합니다. 이 페이지의 예제에서는 llama-2-7b-chat.Q5_K_M.gguf 모델(4.67 GB)을 사용하지만, 여러분의 요구에 가장 잘 맞는 크기, 성능, 정확도의 모델을 찾아보세요.
- 새로운 .NET 프로젝트를 만들고 LLamaSharp와 LLamaSharp.Backend.Cpu NuGet 패키지를 추가합니다. CUDA 백엔드 패키지는 일부 시스템에서 GPU 가속 처리를 지원하지만, 하드웨어 관련 메모리 오류를 피하기 위해 CPU 패키지로 시작하는 것이 좋습니다.
- AI 채팅을 위해 LLamaSharp 클래스를 사용하여 모델을 로드하고 채팅 세션을 시작합니다.